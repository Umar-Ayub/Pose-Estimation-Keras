{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bhandawatlab_duke/.local/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(r'~/Pose-Estimation-Keras')\n",
    "import cnn\n",
    "from cnn import conv_base\n",
    "from build_dataset import Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import stats\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from time import time\n",
    "import cv2\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Hyperparameters\n",
    "total_frames = 5\n",
    "frame_height = 224\n",
    "frame_width = 224\n",
    "frame_channels = 3\n",
    "num_bodyparts = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Model)                (None, 7, 7, 2048)   23587712    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 14, 14, 30)   552990      resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 14, 14, 30)   552990      resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 28, 28, 24)   6504        conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 24)   6504        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 56, 56, 18)   3906        conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 56, 56, 18)   3906        conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 112, 112, 12) 1956        conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 112, 112, 12) 1956        conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "same (Conv2DTranspose)          (None, 224, 224, 6)  654         conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "deconvolution_network_output (C (None, 224, 224, 6)  654         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "deconvolution_fusion_network_ou (None, 224, 224, 1)  55          same[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 24,719,787\n",
      "Trainable params: 24,666,667\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_deconv_model():\n",
    "    frame_tensor = conv_base.layers[0].get_input_at(0)\n",
    "    resnet_output = cnn.convolution_network(frame_tensor)\n",
    "    model_resnet = models.Model(inputs=[frame_tensor], outputs=[resnet_output])\n",
    "    \n",
    "    deconv_output = cnn.deconvolution_network(model_resnet.output)\n",
    "#     model_deconv = models.Model(inputs=[frame_tensor], outputs=[deconv_output])\n",
    "    \n",
    "    \n",
    "    deconv_fusion_output = cnn.deconvolution_fusion_network(model_resnet.output)\n",
    "#     model_deconv_fusion = models.Model(inputs=[frame_tensor], outputs=[deconv_fusion_output])\n",
    "    model = models.Model(inputs=[frame_tensor], outputs=[deconv_output, deconv_fusion_output])\n",
    "    \n",
    "    \n",
    "    losses = {\n",
    "        \"deconvolution_network_output\": \"binary_crossentropy\",\n",
    "        \"deconvolution_fusion_network_output\": \"binary_crossentropy\"\n",
    "#         \"spatial_fusion_output\": \"binary_crossentropy\",\n",
    "#         \"optical_flow_output\": \"binary_crossentropy\"\n",
    "    }\n",
    "    lossWeights = {\"deconvolution_network_output\": 1.0,\n",
    "                   \"deconvolution_fusion_network_output\": 1.0}\n",
    "#                    \"spatial_fusion_output\": 1.0,\n",
    "#                    \"optical_flow_output\": 1.0}\n",
    "\n",
    "    opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "    model.compile( optimizer=opt, loss=losses, loss_weights=lossWeights, metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_deconv_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 4, 6, 7, 8, 11, 15, 16, 17, 18, 19, 20, 23, 120, 126, 127, 178, 185, 199, 228, 244, 261, 267, 285, 306, 335, 367, 375, 389, 405, 410, 415, 420, 426, 430, 436, 440, 445, 452, 455, 460, 465, 470, 476, 480, 485, 490, 496, 500, 511, 520, 525, 530, 535, 542, 552, 555, 568, 570, 575, 581, 588, 595, 600, 605, 610, 615, 621, 625, 633, 636, 640, 645, 650, 655, 665, 670, 675, 680, 685, 690, 695, 700, 705, 710, 715, 718, 723, 725, 751, 755, 770, 783, 784, 804, 812, 816, 823, 829, 838, 841, 845, 850, 855, 863, 865, 891, 894, 904, 908, 915, 920, 929, 946, 953, 955, 959, 965, 970, 980, 987, 990, 995, 1030, 1035, 1041, 1045, 1160, 1164, 1170, 1174, 1220, 1225, 1230, 1235, 1240, 1245, 1250, 1255, 1267, 1275, 1280, 1285, 1290]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def parse_csv(file):\n",
    "    with open(file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        indices = []\n",
    "        for line in csv_reader:\n",
    "            indices.extend([int(f) for f in line if f!=''])\n",
    "    return indices\n",
    "\n",
    "indices = parse_csv('dataset_indices.csv')\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../dataset.h5', 'r') as hf:\n",
    "#     central_heatmap = hf['central_heatmap'][indices]\n",
    "    frames = hf['frames'][indices]\n",
    "    heatmaps_individual = hf['heatmaps_individual'][indices]\n",
    "    heatmaps_combined = hf['heatmaps_combined'][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 5, 224, 224, 6)\n",
      "(146, 5, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "home_dir = os.getcwd()\n",
    "# central_heatmap = np.expand_dims(central_heatmap, axis=-1)\n",
    "heatmaps_combined = np.expand_dims(heatmaps_combined, axis=-1)\n",
    "heatmaps_individual = np.swapaxes(heatmaps_individual, 2, 4)\n",
    "# print(central_heatmap.shape)\n",
    "print(heatmaps_individual.shape)\n",
    "print(heatmaps_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    try:\n",
    "        if epoch < 30:\n",
    "            lrate = 0.0001\n",
    "        elif epoch < 70:\n",
    "            lrate = 0.00005\n",
    "        elif epoch < 200:\n",
    "            lrate = 0.00001\n",
    "        return lrate\n",
    "    except:\n",
    "        print('Error in setting learning rate')\n",
    "lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath= os.path.join(home_dir, \"pretrained_models/model_sep_fusion_11119.h5\"),\n",
    "                               verbose=1,\n",
    "                               monitor='loss')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(home_dir, \"logs/{}\".format(time())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 50s - loss: 0.8924 - deconvolution_network_output_loss: 0.5390 - deconvolution_fusion_network_output_loss: 0.3278 - deconvolution_network_output_acc: 0.7033 - deconvolution_fusion_network_output_acc: 0.9965\n",
      "\n",
      "Epoch 00001: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 2/150\n",
      " - 36s - loss: 0.3474 - deconvolution_network_output_loss: 0.1221 - deconvolution_fusion_network_output_loss: 0.1985 - deconvolution_network_output_acc: 0.9841 - deconvolution_fusion_network_output_acc: 0.9967\n",
      "\n",
      "Epoch 00002: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 3/150\n",
      " - 36s - loss: 0.2696 - deconvolution_network_output_loss: 0.0772 - deconvolution_fusion_network_output_loss: 0.1660 - deconvolution_network_output_acc: 0.9962 - deconvolution_fusion_network_output_acc: 0.9952\n",
      "\n",
      "Epoch 00003: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 4/150\n",
      " - 36s - loss: 0.2338 - deconvolution_network_output_loss: 0.0691 - deconvolution_fusion_network_output_loss: 0.1388 - deconvolution_network_output_acc: 0.9961 - deconvolution_fusion_network_output_acc: 0.9905\n",
      "\n",
      "Epoch 00004: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 5/150\n",
      " - 36s - loss: 0.1942 - deconvolution_network_output_loss: 0.0594 - deconvolution_fusion_network_output_loss: 0.1094 - deconvolution_network_output_acc: 0.9953 - deconvolution_fusion_network_output_acc: 0.9851\n",
      "\n",
      "Epoch 00005: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 6/150\n",
      " - 36s - loss: 0.1633 - deconvolution_network_output_loss: 0.0537 - deconvolution_fusion_network_output_loss: 0.0846 - deconvolution_network_output_acc: 0.9955 - deconvolution_fusion_network_output_acc: 0.9811\n",
      "\n",
      "Epoch 00006: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 7/150\n",
      " - 36s - loss: 0.1480 - deconvolution_network_output_loss: 0.0568 - deconvolution_fusion_network_output_loss: 0.0664 - deconvolution_network_output_acc: 0.9950 - deconvolution_fusion_network_output_acc: 0.9778\n",
      "\n",
      "Epoch 00007: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 8/150\n",
      " - 36s - loss: 0.1136 - deconvolution_network_output_loss: 0.0410 - deconvolution_fusion_network_output_loss: 0.0482 - deconvolution_network_output_acc: 0.9940 - deconvolution_fusion_network_output_acc: 0.9771\n",
      "\n",
      "Epoch 00008: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 9/150\n",
      " - 36s - loss: 0.0929 - deconvolution_network_output_loss: 0.0343 - deconvolution_fusion_network_output_loss: 0.0344 - deconvolution_network_output_acc: 0.9935 - deconvolution_fusion_network_output_acc: 0.9809\n",
      "\n",
      "Epoch 00009: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 10/150\n",
      " - 36s - loss: 0.0906 - deconvolution_network_output_loss: 0.0351 - deconvolution_fusion_network_output_loss: 0.0314 - deconvolution_network_output_acc: 0.9932 - deconvolution_fusion_network_output_acc: 0.9790\n",
      "\n",
      "Epoch 00010: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 11/150\n",
      " - 36s - loss: 0.0688 - deconvolution_network_output_loss: 0.0225 - deconvolution_fusion_network_output_loss: 0.0221 - deconvolution_network_output_acc: 0.9926 - deconvolution_fusion_network_output_acc: 0.9803\n",
      "\n",
      "Epoch 00011: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 12/150\n",
      " - 36s - loss: 0.0505 - deconvolution_network_output_loss: 0.0124 - deconvolution_fusion_network_output_loss: 0.0139 - deconvolution_network_output_acc: 0.9913 - deconvolution_fusion_network_output_acc: 0.9825\n",
      "\n",
      "Epoch 00012: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 13/150\n",
      " - 36s - loss: 0.0380 - deconvolution_network_output_loss: 0.0068 - deconvolution_fusion_network_output_loss: 0.0069 - deconvolution_network_output_acc: 0.9911 - deconvolution_fusion_network_output_acc: 0.9821\n",
      "\n",
      "Epoch 00013: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 14/150\n",
      " - 36s - loss: 0.0247 - deconvolution_network_output_loss: -1.5985e-03 - deconvolution_fusion_network_output_loss: 0.0019 - deconvolution_network_output_acc: 0.9913 - deconvolution_fusion_network_output_acc: 0.9833\n",
      "\n",
      "Epoch 00014: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 15/150\n",
      " - 36s - loss: 0.0052 - deconvolution_network_output_loss: -1.5054e-02 - deconvolution_fusion_network_output_loss: -4.2356e-03 - deconvolution_network_output_acc: 0.9902 - deconvolution_fusion_network_output_acc: 0.9847\n",
      "\n",
      "Epoch 00015: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 16/150\n",
      " - 36s - loss: -1.0876e-02 - deconvolution_network_output_loss: -2.5998e-02 - deconvolution_fusion_network_output_loss: -9.4816e-03 - deconvolution_network_output_acc: 0.9902 - deconvolution_fusion_network_output_acc: 0.9844\n",
      "\n",
      "Epoch 00016: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 17/150\n",
      " - 36s - loss: -1.3635e-02 - deconvolution_network_output_loss: -2.7606e-02 - deconvolution_fusion_network_output_loss: -1.0810e-02 - deconvolution_network_output_acc: 0.9908 - deconvolution_fusion_network_output_acc: 0.9844\n",
      "\n",
      "Epoch 00017: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 18/150\n",
      " - 36s - loss: -3.3364e-02 - deconvolution_network_output_loss: -4.1780e-02 - deconvolution_fusion_network_output_loss: -1.6494e-02 - deconvolution_network_output_acc: 0.9900 - deconvolution_fusion_network_output_acc: 0.9854\n",
      "\n",
      "Epoch 00018: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 19/150\n",
      " - 36s - loss: -4.5065e-02 - deconvolution_network_output_loss: -4.7681e-02 - deconvolution_fusion_network_output_loss: -2.2426e-02 - deconvolution_network_output_acc: 0.9905 - deconvolution_fusion_network_output_acc: 0.9857\n",
      "\n",
      "Epoch 00019: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 20/150\n",
      " - 36s - loss: -4.8106e-02 - deconvolution_network_output_loss: -4.9257e-02 - deconvolution_fusion_network_output_loss: -2.4063e-02 - deconvolution_network_output_acc: 0.9901 - deconvolution_fusion_network_output_acc: 0.9856\n",
      "\n",
      "Epoch 00020: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 21/150\n",
      " - 36s - loss: -7.0235e-02 - deconvolution_network_output_loss: -6.5286e-02 - deconvolution_fusion_network_output_loss: -3.0319e-02 - deconvolution_network_output_acc: 0.9906 - deconvolution_fusion_network_output_acc: 0.9863\n",
      "\n",
      "Epoch 00021: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model_sep_fusion_11119.h5\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-62761c7a548f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                            \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                            \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                            callbacks=[ checkpointer, tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    x = np.array(x, dtype = np.float64)\n",
    "    x -= np.mean(x, dtype = np.float64)\n",
    "    x /= np.std(x, dtype = np.float64)\n",
    "    return x\n",
    "\n",
    "\n",
    "def generator(frames, heatmaps_individual, heatmaps_combined):\n",
    "    while(1):\n",
    "        for j,l,m in zip(frames, heatmaps_individual, heatmaps_combined):\n",
    "#             print(l.shape[0])\n",
    "            for channel in range(l.shape[0]):\n",
    "                l[channel, :, :, :] = normalize(l[channel, :, :, :]) \n",
    "                m[channel, :, :, :] = normalize(m[channel, :, :, :])\n",
    "            # include batch dimension and repeat the matrix at axis 0 num_frames times to match the target dimensionality\n",
    "            yield (j, {\n",
    "#                 \"optical_flow_output\" : np.repeat(np.expand_dims(k, axis=0), 5, 0), \n",
    "                \"deconvolution_network_output\": l,\n",
    "                \"deconvolution_fusion_network_output\": m}) \n",
    "            \n",
    "model.fit_generator(generator(frames,heatmaps_individual, heatmaps_combined),\n",
    "                           steps_per_epoch=100, \n",
    "                           verbose = 1,\n",
    "                           epochs = 150,\n",
    "                           callbacks=[lrate, checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
