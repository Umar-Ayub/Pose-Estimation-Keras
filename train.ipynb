{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bhandawatlab_duke/.local/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(r'~/Pose-Estimation-Keras')\n",
    "import cnn\n",
    "from cnn import conv_base\n",
    "from build_dataset import Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import stats\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from time import time\n",
    "import cv2\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Hyperparameters\n",
    "total_frames = 5\n",
    "frame_height = 224\n",
    "frame_width = 224\n",
    "frame_channels = 3\n",
    "num_bodyparts = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 30)        552990    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 24)        6504      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 56, 56, 18)        3906      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 112, 112, 12)      1956      \n",
      "_________________________________________________________________\n",
      "deconvolution_network_output (None, 224, 224, 6)       654       \n",
      "=================================================================\n",
      "Total params: 24,153,722\n",
      "Trainable params: 24,100,602\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_deconv_model():\n",
    "    frame_tensor = conv_base.layers[0].get_input_at(0)\n",
    "    resnet_output = cnn.convolution_network(frame_tensor)\n",
    "    model_resnet = models.Model(inputs=[frame_tensor], outputs=[resnet_output])\n",
    "    \n",
    "    deconv_output = cnn.deconvolution_network(model_resnet.output)\n",
    "    model_deconv = models.Model(inputs=[frame_tensor], outputs=[deconv_output])\n",
    "    \n",
    "    losses = {\n",
    "        \"deconvolution_network_output\": \"binary_crossentropy\",\n",
    "#         \"spatial_fusion_output\": \"binary_crossentropy\",\n",
    "#         \"optical_flow_output\": \"binary_crossentropy\"\n",
    "    }\n",
    "    lossWeights = {\"deconvolution_network_output\": 1.0,}\n",
    "#                    \"spatial_fusion_output\": 1.0,\n",
    "#                    \"optical_flow_output\": 1.0}\n",
    "    \n",
    "#     opt = optimizers.SGD(lr = 0.0, momentum=0.9, nesterov=True)\n",
    "    opt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "    model_deconv.compile( optimizer=opt, loss=losses, loss_weights=lossWeights, metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model_deconv\n",
    "\n",
    "model = build_deconv_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 4, 6, 7, 8, 11, 15, 16, 17, 18, 19, 20, 23, 120, 126, 127, 178, 185, 199, 228, 244, 261, 267, 285, 306, 335, 367, 375, 389, 405, 410, 415, 420, 426, 430, 436, 440, 445, 452, 455, 460, 465, 470, 476, 480, 485, 490, 496, 500, 511, 520, 525, 530, 535, 542, 552, 555, 568, 570, 575, 581, 588, 595, 600, 605, 610, 615, 621, 625, 633, 636, 640, 645, 650, 655, 665, 670, 675, 680, 685, 690, 695, 700, 705, 710, 715, 718, 723, 725, 751, 755, 770, 783, 784, 804, 812, 816, 823, 829, 838, 841, 845, 850, 855, 863, 865, 891, 894, 904, 908, 915, 920, 929, 946, 953, 955, 959, 965, 970, 980, 987, 990, 995, 1030, 1035, 1041, 1045, 1160, 1164, 1170, 1174, 1220, 1225, 1230, 1235, 1240, 1245, 1250, 1255, 1267, 1275, 1280, 1285, 1290]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def parse_csv(file):\n",
    "    with open(file) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        indices = []\n",
    "        for line in csv_reader:\n",
    "            indices.extend([int(f) for f in line if f!=''])\n",
    "    return indices\n",
    "\n",
    "indices = parse_csv('dataset_indices.csv')\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "with h5py.File('../dataset.h5', 'r') as hf:\n",
    "    central_heatmap = hf['central_heatmap'][indices]\n",
    "    frames = hf['frames'][indices]\n",
    "    heatmaps_individual = hf['heatmaps_individual'][indices]\n",
    "    heatmaps_combined = hf['heatmaps_combined'][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 6, 224, 224, 1)\n",
      "(146, 5, 224, 224, 6)\n",
      "(146, 5, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "home_dir = os.getcwd()\n",
    "central_heatmap = np.expand_dims(central_heatmap, axis=-1)\n",
    "heatmaps_combined = np.expand_dims(heatmaps_combined, axis=-1)\n",
    "heatmaps_individual = np.swapaxes(heatmaps_individual, 2, 4)\n",
    "print(central_heatmap.shape)\n",
    "print(heatmaps_individual.shape)\n",
    "print(heatmaps_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 224, 224, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmaps_individual[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    try:\n",
    "        if epoch < 5:\n",
    "            lrate = 0.01\n",
    "        elif epoch < 70:\n",
    "            lrate = 0.002\n",
    "        elif epoch < 100:\n",
    "            lrate = 0.0002\n",
    "        elif epoch <= 120:\n",
    "            lrate = 0.0001\n",
    "        return lrate\n",
    "    except:\n",
    "        print('Error in setting learning rate')\n",
    "        \n",
    "lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath= os.path.join(home_dir, \"pretrained_models/model010919.h5\"),\n",
    "                               verbose=1,\n",
    "                               monitor='loss')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(home_dir, \"logs/{}\".format(time())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " - 50s - loss: 0.5666 - acc: 0.6885\n",
      "\n",
      "Epoch 00001: saving model to /home/bhandawatlab_duke/Pose-Estimation-Keras/pretrained_models/model010919.h5\n",
      "Epoch 2/150\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    x = np.array(x, dtype = np.float64)\n",
    "    x -= np.mean(x, dtype = np.float64)\n",
    "    x /= np.std(x, dtype = np.float64)\n",
    "    return x\n",
    "\n",
    "\n",
    "def generator(frames, heatmaps):\n",
    "    while(1):\n",
    "        for j,l in zip(frames, heatmaps):\n",
    "#             print(l.shape[0])\n",
    "            for channel in range(l.shape[0]):\n",
    "                l[channel, :, :, :] = normalize(l[channel, :, :, :]) \n",
    "            # include batch dimension and repeat the matrix at axis 0 num_frames times to match the target dimensionality\n",
    "            yield (j, {\n",
    "#                 \"optical_flow_output\" : np.repeat(np.expand_dims(k, axis=0), 5, 0), \n",
    "                       \"deconvolution_network_output\": l}) \n",
    "            \n",
    "model.fit_generator(generator(frames,\n",
    "                                     heatmaps_individual),\n",
    "                           steps_per_epoch=100, \n",
    "                           verbose = 2,\n",
    "                           epochs = 150,\n",
    "                           callbacks=[ checkpointer, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PoseVideo:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.name = self.path.rsplit(\"\\\\\")[-1].split(\".\")[0]\n",
    "        self.clip = mpy.VideoFileClip(self.path).resize((224,224)) \n",
    "        self.predict_data = np.asarray(self.chop_video_into_frames(self.clip)[:-1])\n",
    "        \n",
    "    def chop_video_into_frames(self, clip, frames_per_chop=5):\n",
    "        # gather all frames into a list\n",
    "        frames = [x for x in clip.iter_frames()]\n",
    "        \n",
    "        # chop into pieces and save as 2D list\n",
    "        frames_c = [frames[i * frames_per_chop:(i + 1) * frames_per_chop] for i in range((len(frames) + frames_per_chop - 1) // frames_per_chop )] \n",
    "        return frames_c\n",
    "    \n",
    "\n",
    "def heatmap_to_bodypart_location(heatmap):\n",
    "    return np.unravel_index(np.argmax(heatmap, axis=None), heatmap.shape)\n",
    "    \n",
    "def extract_central_frame(tensor):\n",
    "    return tensor[0]\n",
    "\n",
    "def predict_video(video):\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_video_path = r\"/home/bhandawatlab_duke/deep-pose-predict/181023_1_12_video2.avi\"\n",
    "model_path = r\"/home/bhandawatlab_duke/deep-pose/model_uno.h5\"\n",
    "\n",
    "vid_data = PoseVideo(test_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arr = model.predict_on_batch(vid_data.predict_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "frame = 1\n",
    "bp_no = 3\n",
    "plt.subplot(121)\n",
    "plt.imshow(test_arr[frame][ :, :, bp_no])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(vid_data.predict_data[1][frame])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
